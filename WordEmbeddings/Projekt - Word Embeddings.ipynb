{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e95b78e",
   "metadata": {},
   "source": [
    "# Model Word Embeddings\n",
    "Model Word Embeedings (osadzanie słów) jest techniką wykorzystywaną do reprezentowania tekstu w przetwarzaniu języka naturalnego. W przeciwieństwie do prostszych metod, takich jak bag of words, ma na celu uchwycenie znaczenia słów i kontekstowych relacji między nimi, a nie tylko określenia częstotliwości ich występowania.\n",
    "\n",
    "Cechy Modelu Word Embeedings:\n",
    "1) Poszczególne słowa są reprezentowane jako wektory o wartościach rzeczywistych w określonej przestrzeni wektorowej. Każde słowo jest mapowane na jeden wektor. <br>\n",
    "2) Podobne słowa znajdują się bliżej siebie (podobne jeżeli chodzi o ich znaczenie, sposób użycia w języku i kontekst) . <br>\n",
    "3) Każdy wymiar wektora zawiera określone informacje o znaczeniu lub kontekście słowa. Umożliwia to uchwycenie złożonych relacji między słowami. <br>\n",
    "\n",
    "- Tymczasem w prostszych modelach każde słowo jest reprezentowane jako osobna cecha, a obecność lub częstotliwość słów jest wykorzystywana do budowy macierzy. Różne wyrazy mają różne reprezentacje, niezależnie od tego, jak są używane.\n",
    "\n",
    "## Algorytmy osadzania słów\n",
    "Modele Word Embeedings są zwykle wstępnie przeszkolone przy użyciu metod uczenia bez nadzoru na dużych zbiorach danych. Najpopularniejszymi takimi metodami są: Embedding Layer, Word2Vec, GloVe i FastText. Modele te są trenowane na dużych korpusach tekstowych, co pozwala im uchwycić ogólne relacje semantyczne. <br>\n",
    "\n",
    "1) Embedding Layer: w modelach głębokiego uczenia się mapuje dyskretne słowa lub tokeny na ciągłe wektory, umożliwiając reprezentowanie ich jako gęstych osadzeń w sieci neuronowej. <br>\n",
    "2) Word2Vec: uczy się osadzania słów poprzez przewidywanie wyrazów sąsiadujących wobec słowa docelowego oraz poprzez przewidywanie szerszego kontekstu. W ten sposób wychwytuję relacje semantyczne i podobieństwa. <br>\n",
    "3) GloVe: generuje osadzenia słów, wykorzystując globalne statystyki ich współwystępowania. <br>\n",
    "4) FastText: jest rozszerzeniem Word2Vec, które wprowadza informacje o pod-słowach poprzez reprezentowanie każdego wyrazu jako worka n-gramów znaków, co pozwala na obsługę słów spoza słownika.\n",
    "\n",
    "## Opracowanie modelu\n",
    "Opracowanie modelu na potrzeby tego projektu przebiegało według następujących kroków:\n",
    "1) Ładowanie potrzebnych bibliotek i zbiorów danych. <br> \n",
    "2) Połączenie i przetasowanie zestawów danych. <br>\n",
    "3) Czyszczenie danych tekstowych poprzez usunięcie niepotrzebnych znaków, cyfr, znaków interpunkcyjnych, wszelkich symboli specjalnych oraz konwersje tekstu na małe litery. <br> \n",
    "4) Tokenizacja: podział oczyszczonego tekstu na pojedyncze słowa lub tokeny. <br> \n",
    "5) Usuwanie 'Stopwords' - powszechnie używanych słów, które często pojawiają się w języku, ale nie wnoszą wiele do ogólnego zrozumienia tekstu (np. \"the\", \"is\", \"and\", \"a\", \"an\"). <br> \n",
    "6) Stemming: zredukowanie słów do ich formy podstawowej lub źródłowej, znanej jako \"rdzeń\". <br> \n",
    "7) Dzielenie danych: podział zbioru danych na zestawy treningowe i testowe. Jeden zostanie wykorzystany do wytrenowania modelu, a drugi do oceny jego wydajności. <br> \n",
    "8) Tworzenie osadzeń słów za pomocą algorytmu Word2Vec. <br>\n",
    "9) Konwersja danych tekstowych na osadzenia słów. <br> \n",
    "10) Trenowanie modelu klasyfikacji (Model Support Vector Machines (SVM)). <br> \n",
    "11) Ocena Modelu. <br>\n",
    "12) Załadowanie i przygotowanie nowych danych. <br> \n",
    "13) Wykorzystanie modelu do tworzenia predykcji na podstawie nowych danych. <br>\n",
    "14) Ocena dokładności predykcji.\n",
    "\n",
    "## Na czym polega ten projekt?\n",
    "Celem projektu jest stworzenie modelu, który przewidywałby, czy artykuł jest fake newsem, czy nie, na podstawie jego tytułu. Źródłem danych są dwa zbiory - jeden zawiera wyłącznie prawdziwe artykuły, a drugi wyłącznie fałszywe. Każdy z zestawów zawiera ponad 20 000 rekordów, ale tylko cztery tysiące zostały wykorzystane w projekcie (pierwszy tysiąc z każdego zestawu do uczenia algorytmu i testowania oraz ostatni tysiąc z każdego do tworzenia nowych prognoz)\n",
    "\n",
    "Źródło danych: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0996cc14",
   "metadata": {},
   "source": [
    "# 1. Ładowanie potrzebnych bibliotek i zbiorów danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "940e3216",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Bartosz\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 100)\n",
    "import numpy as np\n",
    "import regex as re\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "\n",
    "True_News = pd.read_csv(\"True.csv\",sep=\",\", nrows=1000) #Pobieram pierwszy tysiąc rekordów ze zbioru z prawdziwymi artykułami\n",
    "Fake_News = pd.read_csv(\"Fake.csv\",sep=\",\", nrows=1000) #Pobieram pierwszy tysiąc rekordów ze zbioru z fałszywymi artykułami\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ae2f69",
   "metadata": {},
   "source": [
    "# 2. Połączenie i przetasowanie zestawów danych:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9fc3462",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dodaję kolumny z etykietami\n",
    "Fake_News['label'] = 'fake'\n",
    "True_News['label'] = 'true'\n",
    "\n",
    "#Biorę tylko kolumnę z tytułami i kolumnę z etykietami\n",
    "True_Text = True_News[['title','label']]\n",
    "Fake_Text = Fake_News[['title','label']]\n",
    "\n",
    "combined_df = pd.concat([Fake_Text, True_Text], ignore_index=True) #łączę obydwa zbiory\n",
    "shuffled_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True) #tasuję połączony zbiór"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e3fb846",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[republican, tax, plan, would, deal, financi, ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[republican, senat, candid, think, dont, belie...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[suprem, court, let, trump, latest, travel, ba...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[trump, spoke, putin, elect, interfer, bad, yo...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[bill, let, peopl, bring, conceal, gun, across...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[white, hous, say, focus, get, lowest, possibl...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[watch, republican, vote, impeach, clinton, ad...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[tillerson, say, disagr, trump, xi, north, korea]</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[principl, power, republican, need, what, righ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[trump, say, tax, confer, go, well, pretti, qu...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label\n",
       "0  [republican, tax, plan, would, deal, financi, ...  true\n",
       "1  [republican, senat, candid, think, dont, belie...  fake\n",
       "2  [suprem, court, let, trump, latest, travel, ba...  true\n",
       "3  [trump, spoke, putin, elect, interfer, bad, yo...  fake\n",
       "4  [bill, let, peopl, bring, conceal, gun, across...  true\n",
       "5  [white, hous, say, focus, get, lowest, possibl...  true\n",
       "6  [watch, republican, vote, impeach, clinton, ad...  fake\n",
       "7  [tillerson, say, disagr, trump, xi, north, korea]  true\n",
       "8  [principl, power, republican, need, what, righ...  fake\n",
       "9  [trump, say, tax, confer, go, well, pretti, qu...  true"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df.head(10)\n",
    "#W ten sposób otrzymuję zbiór, w którym zmieszane są ze sobą tytuły prawdziwych i fałszywych artykułów (po 1000 każdego typu)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b12f9e",
   "metadata": {},
   "source": [
    "# (3, 4, 5, 6). Czyszczenie tekstu, tokenizacja, usuwanie 'stopwords', stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93073f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z\\s]', '', text) #usuwanie znaków niealfabetycznych\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "    return cleaned_text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    filtered_tokens = [token for token in tokens if not token in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def apply_stemming(tokens):\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "shuffled_df['title'] = shuffled_df['title'].apply(clean_text)\n",
    "shuffled_df['title'] = shuffled_df['title'].apply(tokenize_text)\n",
    "shuffled_df['title'] = shuffled_df['title'].apply(remove_stopwords)\n",
    "shuffled_df['title'] = shuffled_df['title'].apply(apply_stemming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "371faaf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title label\n",
      "0  [republican, tax, plan, would, deal, financi, ...  true\n",
      "1  [republican, senat, candid, think, dont, belie...  fake\n",
      "2  [suprem, court, let, trump, latest, travel, ba...  true\n",
      "3  [trump, spoke, putin, elect, interfer, bad, yo...  fake\n",
      "4  [bill, let, peopl, bring, conceal, gun, across...  true\n",
      "5  [white, hous, say, focus, get, lowest, possibl...  true\n",
      "6  [watch, republican, vote, impeach, clinton, ad...  fake\n",
      "7  [tillerson, say, disagr, trump, xi, north, korea]  true\n",
      "8  [principl, power, republican, need, what, righ...  fake\n",
      "9  [trump, say, tax, confer, go, well, pretti, qu...  true\n"
     ]
    }
   ],
   "source": [
    "print(shuffled_df.head(10)) #Tekst tytułów po przygotowaniu wraz z etykietami"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b087dbf3",
   "metadata": {},
   "source": [
    "# 7. Podział zbioru danych na zestawy treningowe i testowe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2df1e878",
   "metadata": {},
   "outputs": [],
   "source": [
    "#podzielenie danych na zbiór treningowy(80%) i zbiór testowy (20%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(shuffled_df['title'], shuffled_df['label'], test_size=0.2, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fc4a74",
   "metadata": {},
   "source": [
    "# 8. Tworzenie osadzeń słów za pomocą alogrytmu Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca0fa8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec_model = Word2Vec(sentences=X_train, vector_size=100, window=5, min_count=1, sg=1)\n",
    "vocab_size = len(word2vec_model.wv.key_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d3ecbd",
   "metadata": {},
   "source": [
    "# 9. Konwersja danych tekstowych na osadzenia słów\n",
    "Definiuję funkcję get_word_embeddings, która konwertuje każdy tokenizowany tekst na osadzenia słów. Jeśli słowo jest obecne w słowniku Word2Vec, jego osadzenie jest dodawane do listy. Jeśli słowo nie występuje, zamiast niego dodawany jest wektor zerowy. Na koniec funkcja oblicza średnią z osadzeń na liście. \n",
    "\n",
    "Stosuję tę funkcję wobec danych tekstowych w zestawach treningowych i tekstowych. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "128b49a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.0966473   0.08888631  0.0325212  ... -0.11249717  0.05809769\n",
      "  -0.01644182]\n",
      " [-0.08513754  0.07907382  0.02735907 ... -0.10137569  0.05658038\n",
      "  -0.01768831]\n",
      " [-0.16399764  0.15831263  0.05542858 ... -0.19794957  0.11429127\n",
      "  -0.03993649]\n",
      " ...\n",
      " [-0.07024146  0.06985366  0.02856467 ... -0.08635706  0.0447177\n",
      "  -0.01479542]\n",
      " [-0.16832042  0.15781893  0.06029854 ... -0.1949763   0.10774559\n",
      "  -0.03133705]\n",
      " [-0.06895089  0.06145653  0.02849704 ... -0.08077771  0.04654139\n",
      "  -0.01379546]]\n",
      "[[-0.13468651  0.12096632  0.04553999 ... -0.15294982  0.08649515\n",
      "  -0.02894375]\n",
      " [-0.1667513   0.15606095  0.05697342 ... -0.1953565   0.11311278\n",
      "  -0.03743687]\n",
      " [-0.12492042  0.11530716  0.04299495 ... -0.14490327  0.07972731\n",
      "  -0.02593122]\n",
      " ...\n",
      " [-0.16768192  0.1556727   0.05702152 ... -0.19337837  0.10955086\n",
      "  -0.02906619]\n",
      " [-0.14824313  0.13634394  0.0491188  ... -0.17336045  0.09347238\n",
      "  -0.03243966]\n",
      " [-0.1162855   0.11074095  0.04447782 ... -0.14230995  0.08149279\n",
      "  -0.02530337]]\n"
     ]
    }
   ],
   "source": [
    "def get_word_embeddings(text):\n",
    "    embeddings = []\n",
    "    for word in text:\n",
    "        if word in word2vec_model.wv:\n",
    "            embeddings.append(word2vec_model.wv[word])\n",
    "    if len(embeddings) == 0:\n",
    "        embeddings.append(np.zeros(word2vec_model.vector_size))\n",
    "    return np.mean(embeddings, axis=0)\n",
    "\n",
    "X_train_embeddings = np.array([get_word_embeddings(text) for text in X_train])\n",
    "X_test_embeddings = np.array([get_word_embeddings(text) for text in X_test])\n",
    "\n",
    "# Wyniki osadzenia\n",
    "print(X_train_embeddings)\n",
    "print(X_test_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac01360",
   "metadata": {},
   "source": [
    "# 10. Trenowanie modelu klasyfikacji (Model Support Vector Machines (SVM))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06d21637",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = SVC()\n",
    "classifier.fit(X_train_embeddings, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcf35bbb",
   "metadata": {},
   "source": [
    "# 11. Ocena Modelu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc11b30a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.605\n"
     ]
    }
   ],
   "source": [
    "accuracy = classifier.score(X_test_embeddings, y_test)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "#60% dokładności na zbiorze testowym "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05aa71",
   "metadata": {},
   "source": [
    "# 12. Załadowanie i przygotowanie nowych danych"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "60990a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "True_News_New = pd.read_csv(\"True.csv\",sep=\",\", skiprows=lambda x: x != 0 and x < (1000 - 1), nrows=1000)\n",
    "Fake_News_New = pd.read_csv(\"Fake.csv\",sep=\",\", skiprows=lambda x: x != 0 and x < (1000 - 1), nrows=1000)\n",
    "\n",
    "Fake_News_New['label'] = 'fake'\n",
    "True_News_New['label'] = 'true'\n",
    "\n",
    "True_Text_New = True_News_New[['title','label']]\n",
    "Fake_Text_New = Fake_News_New[['title','label']]\n",
    "\n",
    "combined_df_New = pd.concat([Fake_Text_New, True_Text_New], ignore_index=True)\n",
    "shuffled_df_New = combined_df_New.sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a6c82a22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Facebook gives election ad data to U.S. specia...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Under The GOP’s Health Care Bill, Premiums Co...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>U.S. businesses fear NAFTA doomed; Mexico warn...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inventor Of The Worldwide Web Just DESTROYED ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Republicans eye alternatives for getting to 20...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Mexico says NAFTA would survive with Canada ev...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Republican’s Excuse For Cutting Food Stamps: ...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Trump says violence by anti-fascists proves hi...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Dem Challenger To Paul Ryan Has Raised A Mass...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Trump says he's likely to sign healthcare orde...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label\n",
       "0  Facebook gives election ad data to U.S. specia...  true\n",
       "1   Under The GOP’s Health Care Bill, Premiums Co...  fake\n",
       "2  U.S. businesses fear NAFTA doomed; Mexico warn...  true\n",
       "3   Inventor Of The Worldwide Web Just DESTROYED ...  fake\n",
       "4  Republicans eye alternatives for getting to 20...  true\n",
       "5  Mexico says NAFTA would survive with Canada ev...  true\n",
       "6   Republican’s Excuse For Cutting Food Stamps: ...  fake\n",
       "7  Trump says violence by anti-fascists proves hi...  true\n",
       "8   Dem Challenger To Paul Ryan Has Raised A Mass...  fake\n",
       "9  Trump says he's likely to sign healthcare orde...  true"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df_New.head(10) #nowe dane przed przygotowaniem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d5c399",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[facebook, give, elect, ad, data, us, special,...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gop, health, care, bill, premium, could, rise...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[us, busi, fear, nafta, doom, mexico, warn, co...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[inventor, worldwid, web, destroy, trump, disg...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[republican, eye, altern, get, percent, corpor...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[mexico, say, nafta, would, surviv, canada, ev...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[republican, excus, cut, food, stamp, bibl, sa...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[trump, say, violenc, antifascist, prove, righ...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[dem, challeng, paul, ryan, rais, massiv, amou...</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[trump, say, he, like, sign, healthcar, order,...</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title label\n",
       "0  [facebook, give, elect, ad, data, us, special,...  true\n",
       "1  [gop, health, care, bill, premium, could, rise...  fake\n",
       "2  [us, busi, fear, nafta, doom, mexico, warn, co...  true\n",
       "3  [inventor, worldwid, web, destroy, trump, disg...  fake\n",
       "4  [republican, eye, altern, get, percent, corpor...  true\n",
       "5  [mexico, say, nafta, would, surviv, canada, ev...  true\n",
       "6  [republican, excus, cut, food, stamp, bibl, sa...  fake\n",
       "7  [trump, say, violenc, antifascist, prove, righ...  true\n",
       "8  [dem, challeng, paul, ryan, rais, massiv, amou...  fake\n",
       "9  [trump, say, he, like, sign, healthcar, order,...  true"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffled_df_New['title'] = shuffled_df_New['title'].apply(clean_text)\n",
    "shuffled_df_New['title'] = shuffled_df_New['title'].apply(tokenize_text)\n",
    "shuffled_df_New['title'] = shuffled_df_New['title'].apply(remove_stopwords)\n",
    "shuffled_df_New['title'] = shuffled_df_New['title'].apply(apply_stemming)\n",
    "\n",
    "new_data = shuffled_df_New['title']\n",
    "actual_labels = shuffled_df_New['label']\n",
    "\n",
    "shuffled_df_New.head(10) #nowe dane po przygotowaniu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c64be4",
   "metadata": {},
   "source": [
    "# 13. Wykorzystanie modelu do tworzenia predykcji na podstawie nowych danych."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4f102e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data_embeddings = [get_word_embeddings(text) for text in new_data ]\n",
    "predictions = classifier.predict(new_data_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a337a61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for data, prediction in zip(new_data , predictions):\n",
    "#     print(f\"Data: {data}\")\n",
    "#     print(f\"Prediction: {prediction}\")\n",
    "#     print()\n",
    "\n",
    "#Predykcje dotyczące poszczególnych tytułów"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b289dd7",
   "metadata": {},
   "source": [
    "# 14. Ocena dokładności predykcji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c6b59742",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Data</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[facebook, give, elect, ad, data, us, special,...</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[gop, health, care, bill, premium, could, rise...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[us, busi, fear, nafta, doom, mexico, warn, co...</td>\n",
       "      <td>fake</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[inventor, worldwid, web, destroy, trump, disg...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[republican, eye, altern, get, percent, corpor...</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[mexico, say, nafta, would, surviv, canada, ev...</td>\n",
       "      <td>fake</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[republican, excus, cut, food, stamp, bibl, sa...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[trump, say, violenc, antifascist, prove, righ...</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[dem, challeng, paul, ryan, rais, massiv, amou...</td>\n",
       "      <td>fake</td>\n",
       "      <td>fake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[trump, say, he, like, sign, healthcar, order,...</td>\n",
       "      <td>true</td>\n",
       "      <td>true</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Data Prediction Actual\n",
       "0  [facebook, give, elect, ad, data, us, special,...       true   true\n",
       "1  [gop, health, care, bill, premium, could, rise...       fake   fake\n",
       "2  [us, busi, fear, nafta, doom, mexico, warn, co...       fake   true\n",
       "3  [inventor, worldwid, web, destroy, trump, disg...       fake   fake\n",
       "4  [republican, eye, altern, get, percent, corpor...       true   true\n",
       "5  [mexico, say, nafta, would, surviv, canada, ev...       fake   true\n",
       "6  [republican, excus, cut, food, stamp, bibl, sa...       fake   fake\n",
       "7  [trump, say, violenc, antifascist, prove, righ...       true   true\n",
       "8  [dem, challeng, paul, ryan, rais, massiv, amou...       fake   fake\n",
       "9  [trump, say, he, like, sign, healthcar, order,...       true   true"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = pd.DataFrame({'Data': new_data, 'Prediction': predictions, 'Actual': actual_labels})\n",
    "results_df.head(10) #Porównanie predykcji ('Prediction') z rzeczywistymi etykietami ('Actual') dla poszczególnych artykułów "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8a63e351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.562\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(actual_labels, predictions)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "#56% dokładności na mieszanym zbiorze prawdziwych i fałszywych artykułów (w porównaniu do 60% na zbiorze testowym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c724f61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
